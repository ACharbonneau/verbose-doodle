# Deliverables by Item for Common Fund Data Ecosystem Workplan 

### PROVIDING COMMUNICATION COORDINATION AND SYNCHRONIZATION; White, Brown
- (Brown)  A website that hosts all of the currently available training materials, information about upcoming events, Use Cases, and links to the submission portal. This site will be maintained primarily by the Davis team, with oversight by the NIH via monthly content meetings with Jen Yttri.
- (Brown)  Coordinate regular consortium events to facilitate interactions between the sub-awardees and the DCCs. Individual meeting formats will be either face to face or by video conference as determined by the agenda and goals for a given meeting.
- (White) Provide oversight, guidance and organization support for between-group interactions (e.g., all sub-awardees) for the project in the form of project management, establishing priorities, as well as defining, attending and assisting with conference calls and face-to-face meetings. 

### DEVELOPING AN ENGAGEMENT PROCESS; Brown, White
- (Brown)  A set of Use Cases that encapsulate the shared goals of the Common Fund Data Ecosystem and the DCCs and that can be used to guide design and programming decisions. 
- (White)  We will identify areas of synergy with all DCCs, assess their resources and needs, define strategic goals for collaboration and develop common workplans. We will also work with DCCs to meet requirements for new data sets and metadata levels with enhancements and modifications to the C2M2.
METADATA LEVELS; White, Kesselman, Sansone
- (Sansone)  Document describing metadata and FAIR maturity levels (see item 20)
- (White) Assist in documentation and distribution of all metadata and FAIR maturity levels. 
- (White) Extensible Python-based tools for ingesting data into C2M2 model. 
- (Sansone) Serve as lead developers and points of contact for the underlying C2M2 data model referred to as DATS.
- (White)  Participate with Kesselman on the specification and implementation of C2M2 with submission levels in order to support data submission to the Common Fund Data Ecosystem.
- (Kesselman)  Participate with White on the specification and implementation of C2M2 with submission levels in order to support data submission to the Common Fund Data Ecosystem.
### COMMON FUND DATA ECOSYSTEM QUERY PORTAL; White, Kesselman, Ma’ayan
- (White/Kesselman)  Deploy free-text and faceted-search query interfaces
- (Ma’ayan) Assist in the integration of FAIRness measures into the query portal.
### PIPELINE REPOSITORY; Ma’ayan
- (Ma’ayan)  A database that consolidates information about scripts, workflows, and pipelines developed by the CF DCCs to process their data. 
### TRAINING; Brown
NOTE: All teams will contribute training materials in the form of documentation as well as electronic media as part of the deliverables described elsewhere in this document. 
D16. - (Brown)  A set of training materials including webinars and documentation for DCCs on the use of the data dashboard.
- (Brown)  Training materials in the form of webinars, face-to-face seminars and documentation for end-users to utilize the Common Fund Data Ecosystem resources. 
- (Brown)  A set of training materials including training and documentation for DCCs on the use of the FAIRshake.
### METADATA HARMONIZATION; White, Sansone
- (White) Survey each DCC to Identify and document ontologies and vocabularies used, leveraging FAIRsharing registry, develop approaches for harmonization across all centers. 
- (White) Harmonized metadata at various levels of maturity; 
- (Sansone) Contribute to a FAIR cookbook on how to best achieve metadata FAIRness (which will feed into item 26)
### DATA-LEVEL HARMONIZATION; White, Brown, Ma’ayan
- (Ma’ayan) Several uniformly processed CF datasets from at least two CF programs. Open source workflows used to process the data. High level processed data provided with rich metadata in a dataframe format that can be consumed by R or Python data science common data analysis libraries. 
- (Brown)  A set of Use Cases that document the workflows currently used by DCC users, and the analyses they would do given a repository like the Common Fund Data Ecosystem. These Use Cases will guide the selection and features of processed datasets.
- (White) Provide technical assistance in the application of open source workflows on Common Fund Data Ecosystem datasets to ensure uniformity of similar datatypes. 
### FAIRNESS ASSESSMENT; Ma’ayan, Brown, Sansone
- (Ma’ayan) An updated version of FAIRshake that contains all the Common Fund program digital objects including datasets, tools and workflows and their FAIR assessments as the Common Fund datasets come on line.
- (Sansone) Connect FAIRsharing and FAIRshake, to provide lists of accepted values for identifiers schemas and metadata standards, during the FAIR assessment.
- (Brown) Document describing the initial level of FAIRness for participating DCC’s as well as improvements in FAIRness.
### DATA STAGING PROCESS DEVELOPMENT AND DOCUMENTATION; White, 
- (White)  Dataset onboarding documentation for incoming dataset groups. Documentation and best practice recommendations for FISMA compliance 
